# -*- coding: utf-8 -*-
"""Proyek_Time_Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tydnn2wyyIbFwWZ_vmXqeVDLd-vRjibx
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Dense, LSTM
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential

# Membuat folder .kaggle di dalam folder root
!rm -rf ~/.kaggle && mkdir ~/.kaggle/

# Menyalin berkas kaggle.json pada direktori aktif saat ini ke folder .kaggle
!mv kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

# mendownload dataset menggunakan API command dari kaggle
!kaggle datasets download -d szrlee/stock-time-series-20050101-to-20171231

# mengekstrak file zip
import zipfile
zip_file = zipfile.ZipFile('/content/stock-time-series-20050101-to-20171231.zip')
zip_file.extractall('/content/')

df = pd.read_csv ('AABA_2006-01-01_to_2018-01-01.csv')
df

df.isnull().sum()

category = pd.get_dummies(df.Name)
new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='Name')
new_df

dates = df['Date'].values
volume = df['Volume'].values

plt.figure(figsize=(15, 5))
plt.plot(dates, volume)
plt.title('Time Series',
          fontsize=20);

#Normalisasi
from sklearn.preprocessing import MinMaxScaler
new_data = df.filter(['Volume'])

dataset = new_data.values
scaler = MinMaxScaler(feature_range=(0,1))
scaled = scaler.fit_transform(dataset)

scaled

X_train, X_test, y_train,y_test = train_test_split(dates, scaled, test_size=0.2)

y_train = y_train.astype(np.float32)
y_test = y_test.astype(np.float32)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(window_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:-1], w[-1:]))
  return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(y_train, window_size=60, batch_size=100, shuffle_buffer=1000)
val_set = windowed_dataset(y_test, window_size=60, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(60, return_sequences=True, input_shape=[None, 1]),
    tf.keras.layers.LSTM(60),
    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1),
])

class myCallback(tf.keras.callbacks.Callback): 
 def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<0.1) and (logs.get('val_mae')<0.10):
      print("\n Mae dibawah <10%!") 
      self.model.stop_training = True 
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_set,
                    epochs=100, validation_data=val_set, callbacks=[callbacks])

plt.plot(history.history['mae'])
plt.title('Mae dan Validation Model')
plt.ylabel('Mae')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show

plt.plot(history.history['loss'])
plt.title('Loss Model')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show